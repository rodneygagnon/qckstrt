/**
 * LLM Module
 *
 * Configures Dependency Injection for language model providers.
 *
 * Provider: Ollama (self-hosted, OSS, full privacy)
 *
 * Supports any Ollama model:
 * - falcon (default, 7B, TII's open-source model)
 * - llama3.2 (3B, fast and efficient)
 * - mistral (7B, excellent quality)
 * - llama3.1 (8B, latest Llama)
 * - Or any other model from ollama.ai/library
 *
 * Setup:
 * 1. Install Ollama: https://ollama.ai
 * 2. Pull model: ollama pull falcon
 * 3. Start server: ollama serve
 */
export declare class LLMModule {}
//# sourceMappingURL=llm.module.d.ts.map
