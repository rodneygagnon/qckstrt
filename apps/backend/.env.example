PROJECT='qckstrt'
NODE_ENV='development'

AWS_REGION='us-west-2'
# AWS_PROFILE='qckstrt'
AWS_SECRETS='qckstrt-dev-local-keys'

PORT=3001
APPLICATION='vectors'
VERSION='0.0.1'
DESCRIPTION='OCR/AI Functionality'

# Embeddings Configuration (OSS default)
EMBEDDINGS_PROVIDER='xenova'
EMBEDDINGS_CHUNK_SIZE=1000
EMBEDDINGS_CHUNK_OVERLAP=200

# Xenova Configuration (OSS, in-process, zero setup)
EMBEDDINGS_XENOVA_MODEL='Xenova/all-MiniLM-L6-v2'
# Other options: 'Xenova/paraphrase-MiniLM-L3-v2' (smaller/faster)

# Ollama Configuration (OSS, requires ollama server)
# EMBEDDINGS_PROVIDER='ollama'
# EMBEDDINGS_OLLAMA_URL='http://localhost:11434'
# EMBEDDINGS_OLLAMA_MODEL='nomic-embed-text'

# LLM Configuration (OSS, self-hosted via Ollama)
LLM_URL='http://localhost:11434'
LLM_MODEL='falcon'
# Other options: llama3.2, mistral, llama3.1, qwen2.5, etc.
# See available models at: https://ollama.ai/library

# Vector Database Configuration
VECTOR_DB_PROVIDER='chromadb'
VECTOR_DB_CHROMA_URL='http://localhost:8000'

# Relational Database Configuration
# Development default: SQLite (zero setup)
RELATIONAL_DB_PROVIDER='sqlite'
RELATIONAL_DB_DATABASE='./data/dev.sqlite'

# Production: PostgreSQL
# RELATIONAL_DB_PROVIDER='postgres'
# RELATIONAL_DB_HOST='localhost'
# RELATIONAL_DB_PORT=5432
# RELATIONAL_DB_DATABASE='qckstrt'
# RELATIONAL_DB_USERNAME='qckstrt_user'
# RELATIONAL_DB_PASSWORD='qckstrt_password'
# RELATIONAL_DB_SSL=false
